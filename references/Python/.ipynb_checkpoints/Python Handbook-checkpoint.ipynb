{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Handbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "**Illustration of seaborn historgrams and pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(\"./data.csv\")\n",
    "\n",
    "# data_proc = pd.DataFrame(data[\"class\"].apply(int))\n",
    "# Drop columns\n",
    "# data = data.drop(columns=[\"veil-type\", \"class\"])\n",
    "\n",
    "# Examine data\n",
    "data.describe()\n",
    "\n",
    "# Convert to factors\n",
    "# convert to categorical variables\n",
    "for i in range(21):\n",
    "    df.iloc[:,i] = df.iloc[:,i].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of each categor against frequency\n",
    "fig, axes =plt.subplots(1, 2, figsize=(15,15))\n",
    "sns.countplot(y = data[\"cap-color\"], data = data, ax=axes[0][0])\n",
    "sns.countplot(y = data.odor, data = data, ax=axes[0][1])\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of count of each type of category with respect to the class\n",
    "\n",
    "fig, axes =plt.subplots(1, 2, figsize=(15,15))\n",
    "sns.countplot(y = data[\"cap-color\"], hue = \"class\", data = data, ax=axes[0][0])\n",
    "sns.countplot(y = data.odor, hue = \"class\", data = data, ax=axes[0][1])\n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of poison-ness per odour class\n",
    "data[\"odor\"].value_counts()/data[\"odor\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage edible for each category within each variable\n",
    "# odour, spore print colour and population \n",
    "data_odor = pd.crosstab(data[\"class\"], df.odor)\n",
    "data_odor/data_odor.sum()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "\n",
    "- **Gradient boosting machine**. To estimate the best $f^{*}$ we start with $f^{(0)}(x) = 0$ and do\n",
    "$$\n",
    "f^{(t+1)} = f^{(t)} + \\gamma G^{(t+1)},\n",
    "$$\n",
    "where $G^{(t+1)}$ is the weak learner fitted. At each iteration $t$, the GBM fits a small tree to the current residuals, and then adds this tree to the current model with a small weight $\\gamma$.\n",
    "\n",
    "- Use small trees avoid fitting overly complex models and due to the curse of dimensionality.\n",
    "- $\\gamma$ to use shrinkage to learn slowly. When $\\gamma$ is large the GD can converge faster to minimum. When the number of iterations $T$ is high the boosting can reduce the training error more $\\Rightarrow$ take large $T$ and small $\\gamma$.\n",
    "- Can also do subsampling (ask Harrison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of GBM\n",
    "from lightgbm import LGBMClassifier\n",
    "import sklearn\n",
    "\n",
    "# Fit model\n",
    "clf = LGBMClassifier(num_leaves = 3, n_estimators = 200, scale_pos_weight = 79246 / 1272)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Generate importance plots \n",
    "import lightgbm as lgb\n",
    "p = lgb.plot_importance(clf.booster_, figsize = (10,50))\n",
    "\n",
    "# Validation\n",
    "print(\"AUC:\", sklearn.metrics.roc_auc_score(y_test, clf.predict(x_test)) * 100)\n",
    "\n",
    "print( sklearn.metrics.confusion_matrix(y_test2, clf2.predict(x_test2)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "- PCA for preprocessing\n",
    "    - Useful when model is slow to fit when $p$, number of columns of design matrix, is large\n",
    "    - Multicollinearity\n",
    "    - Using non-parametric methods when $p$ large.\n",
    "- PCR = principal component regression.\n",
    "- K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
